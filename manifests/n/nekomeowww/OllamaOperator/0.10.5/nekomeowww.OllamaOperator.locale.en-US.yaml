# Created with YamlCreate.ps1 Dumplings Mod
# yaml-language-server: $schema=https://aka.ms/winget-manifest.defaultLocale.1.9.0.schema.json

PackageIdentifier: nekomeowww.OllamaOperator
PackageVersion: 0.10.5
PackageLocale: en-US
Publisher: Neko
PublisherUrl: https://github.com/nekomeowww
PublisherSupportUrl: https://github.com/nekomeowww/ollama-operator/issues
PackageName: Ollama Operator
PackageUrl: https://ollama-operator.ayaka.io/
License: Apache-2.0
LicenseUrl: https://github.com/nekomeowww/ollama-operator/blob/HEAD/LICENSE
ShortDescription: "Yet another operator for running large language models on Kubernetes with ease. Powered by Ollama! \U0001F42B"
Description: |-
  While Ollama is a powerful tool for running large language models locally, and the user experience of CLI is just the same as using Docker CLI, it's not possible yet to replicate the same user experience on Kubernetes, especially when it comes to running multiple models on the same cluster with loads of resources and configurations.
  That's where the Ollama Operator kicks in:
  - Install the operator on your Kubernetes cluster
  - Apply the needed CRDs
  - Create your models
  - Wait for the models to be fetched and loaded, that's it!
  Thanks to the great works of lama.cpp, no more worries about Python environment, CUDA drivers.
  The journey to large language models, AIGC, localized agents, ðŸ¦œðŸ”— Langchain and more is just a few steps away!
Moniker: kollama
Tags:
- cluster
- container
- docker
- k8s
- kubernetes
- ollama
ReleaseNotes: |-
  Changelog
  - cbdfae9cfa460173a45b5939d124c991b8e65b34 release: v0.10.5
ReleaseNotesUrl: https://github.com/nekomeowww/ollama-operator/releases/tag/v0.10.5
ManifestType: defaultLocale
ManifestVersion: 1.9.0

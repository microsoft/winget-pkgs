# Created with YamlCreate.ps1 v2.4.3 $debug=NVS1.CRLF.7-4-6.Win32NT
# yaml-language-server: $schema=https://aka.ms/winget-manifest.defaultLocale.1.9.0.schema.json

PackageIdentifier: nekomeowww.OllamaOperator
PackageVersion: 0.10.1
PackageLocale: en-US
Publisher: Neko
PublisherUrl: https://github.com/nekomeowww
PublisherSupportUrl: https://github.com/nekomeowww/ollama-operator/issues
# PrivacyUrl:
# Author:
PackageName: Ollama Operator
PackageUrl: https://ollama-operator.ayaka.io/
License: Apache-2.0
LicenseUrl: https://github.com/nekomeowww/ollama-operator/blob/HEAD/LICENSE
# Copyright:
# CopyrightUrl:
ShortDescription: Yet another operator for running large language models on Kubernetes with ease. Powered by Ollama! üê´
Description: |-
  While Ollama is a powerful tool for running large language models locally, and the user experience of CLI is just the same as using Docker CLI, it's not possible yet to replicate the same user experience on Kubernetes, especially when it comes to running multiple models on the same cluster with loads of resources and configurations.
  That's where the Ollama Operator kicks in:
  - Install the operator on your Kubernetes cluster
  - Apply the needed CRDs
  - Create your models
  - Wait for the models to be fetched and loaded, that's it!
  Thanks to the great works of lama.cpp, no more worries about Python environment, CUDA drivers.
  The journey to large language models, AIGC, localized agents, ü¶úüîó Langchain and more is just a few steps away!
Moniker: kollama
Tags:
- cluster
- container
- docker
- k8s
- kubernetes
- ollama
ReleaseNotes: |-
  Changelog
  - 6eae886 release: v0.10.1
ReleaseNotesUrl: https://github.com/nekomeowww/ollama-operator/releases/tag/v0.10.1
# PurchaseUrl:
# InstallationNotes:
# Documentations:
ManifestType: defaultLocale
ManifestVersion: 1.9.0

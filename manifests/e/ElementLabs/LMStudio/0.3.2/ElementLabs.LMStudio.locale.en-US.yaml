# Created with YamlCreate.ps1 v2.4.1 $debug=NVS2.CRLF.7-4-5.Win32NT
# yaml-language-server: $schema=https://aka.ms/winget-manifest.defaultLocale.1.6.0.schema.json

PackageIdentifier: ElementLabs.LMStudio
PackageVersion: 0.3.2
PackageLocale: en-US
Publisher: LM Studio
PublisherUrl: https://lmstudio.ai/
# PublisherSupportUrl:
# PrivacyUrl:
Author: Element Labs, Inc.
PackageName: LM Studio
PackageUrl: https://lmstudio.ai/
License: Freeware
LicenseUrl: https://lmstudio.ai/terms
Copyright: Â© LM Studio 2023 - 2024
CopyrightUrl: https://lmstudio.ai/terms
ShortDescription: Discover, download, and run local LLMs
Description: LM Studio is an easy to use desktop app for experimenting with local and open-source Large Language Models (LLMs). The LM Studio cross platform desktop app allows you to download and run any ggml-compatible model from Hugging Face, and provides a simple yet powerful model configuration and inferencing UI. The app leverages your GPU when possible.
# Moniker:
Tags:
- ai
- large-language-model
- llama
- llm
- mistral
ReleaseNotes: |-
  What's new in 0.3.2
  - New: Ability to pin models to the top is back!
    - Right-click on a model in My Models and select "Pin to top" to pin it to the top of the list.
  - Chat migration dialog now appears in the chat side bar.
    - You can migrate your chats from pre-0.3.0 versions from there.
    - As of v0.3.1, system prompts now migrated as well.
    - Your old chats are NOT deleted.
  - Don't show a badge with a number on the downloads button if there are no downloads
  - Added a button to collapse the FAQ sidebar in the Discover tab
  - Reduced default context size from 8K to 4K tokens to alleviate Out of Memory issues
    - You can still configure any context size you want in the model load settings
  - Added a warning next to the Flash Attention in model load settings
    - Flash Attention is experimental and may not be suitable for all models
  - Updated the bundled llama.cpp engine to 3246fe84d78c8ccccd4291132809236ef477e9ea (Aug 27)

  Bug Fixes
  - Bug fix: My Models model size aggregation was incorrect when you had multi-part model files
  - Bug fix: (Linux) RAG would fail due to missing bundled embedding model (fixed)
  - Bug fix: Flash Attention - KV cache quantization is back to FP16 by default
    - In 0.3.0, the K and V were both set to Q8, which introduced large latencies in some cases
      - You might notice an increase in memory consumption when FA is ON compared with 0.3.1, but on par with 0.2.31
  - Bug fix: On some setups app would hang at start up (fix + mitigation)
  - Bug fix: Fixed an issue where the downloads panel was dragged over the app top bar
  - Bug fix: Fixed typos in built-in code snippets (server tab)
ReleaseNotesUrl: https://lmstudio.ai/blog
# PurchaseUrl:
# InstallationNotes:
Documentations:
- DocumentLabel: Documentation
  DocumentUrl: https://lmstudio.ai/docs/welcome
ManifestType: defaultLocale
ManifestVersion: 1.6.0

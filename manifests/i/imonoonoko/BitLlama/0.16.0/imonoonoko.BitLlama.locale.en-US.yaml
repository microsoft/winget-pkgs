# Created with WinGet Releaser using komac v2.15.0
# yaml-language-server: $schema=https://aka.ms/winget-manifest.defaultLocale.1.12.0.schema.json

PackageIdentifier: imonoonoko.BitLlama
PackageVersion: 0.16.0
PackageLocale: en-US
Publisher: imonoonoko
PublisherUrl: https://github.com/imonoonoko
PublisherSupportUrl: https://github.com/imonoonoko/Bit-TTT-Engine/issues
PackageName: BitLlama
PackageUrl: https://github.com/imonoonoko/Bit-TTT-Engine
License: MIT
LicenseUrl: https://github.com/imonoonoko/Bit-TTT-Engine/blob/HEAD/LICENSE
ShortDescription: Pure Rust LLM inference engine with 1.58-bit ternary support and Test-Time Training
Description: |-
  BitLlama is a Pure Rust LLM inference engine featuring 1.58-bit ternary quantization,
  Test-Time Training (TTT), Soul learning system, MCP server/client, and private RAG.
  Supports Llama, Gemma, Mistral, Qwen, and BitNet models.
  OpenAI-compatible API server included.
Tags:
- ai
- cli
- inference
- llm
- machine-learning
- rust
ReleaseNotes: 'Full Changelog: v0.15.0...v0.16.0'
ReleaseNotesUrl: https://github.com/imonoonoko/Bit-TTT-Engine/releases/tag/v0.16.0
ManifestType: defaultLocale
ManifestVersion: 1.12.0

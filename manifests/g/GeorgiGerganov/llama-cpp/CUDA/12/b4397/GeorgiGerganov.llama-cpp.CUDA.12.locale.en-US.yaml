# Created with YamlCreate.ps1 v2.4.3 $debug=NVS1.CRLF.7-4-6.Win32NT
# yaml-language-server: $schema=https://aka.ms/winget-manifest.defaultLocale.1.9.0.schema.json

PackageIdentifier: GeorgiGerganov.llama-cpp.CUDA.12
PackageVersion: b4397
PackageLocale: en-US
Publisher: Georgi Gerganov
PublisherUrl: https://ggerganov.com/
PublisherSupportUrl: https://github.com/ggerganov/llama.cpp/issues
# PrivacyUrl:
Author: Georgi Gerganov
PackageName: llama.cpp (CUDA 12 Backend)
PackageUrl: https://github.com/ggerganov/llama.cpp
License: MIT
LicenseUrl: https://github.com/ggerganov/llama.cpp/blob/HEAD/LICENSE
Copyright: Copyright (c) 2023-2024 The ggml authors
# CopyrightUrl:
ShortDescription: LLM inference in C/C++ with CUDA 12 backend
# Description:
# Moniker:
Tags:
- ai
- bert
- deepseek
- large-language-model
- llama
- llm
- qwen
- roberta
# ReleaseNotes:
ReleaseNotesUrl: https://github.com/ggerganov/llama.cpp/releases/tag/b4397
# PurchaseUrl:
# InstallationNotes:
Documentations:
- DocumentLabel: Wiki
  DocumentUrl: https://github.com/ggerganov/llama.cpp/wiki
ManifestType: defaultLocale
ManifestVersion: 1.9.0

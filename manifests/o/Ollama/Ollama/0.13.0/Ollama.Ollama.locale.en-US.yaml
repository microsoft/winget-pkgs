# Created with YamlCreate.ps1 Dumplings Mod
# yaml-language-server: $schema=https://aka.ms/winget-manifest.defaultLocale.1.10.0.schema.json

PackageIdentifier: Ollama.Ollama
PackageVersion: 0.13.0
PackageLocale: en-US
Publisher: Ollama
PublisherUrl: https://ollama.com/
PublisherSupportUrl: https://github.com/ollama/ollama/issues
PackageName: Ollama
PackageUrl: https://ollama.com/
License: MIT
LicenseUrl: https://github.com/ollama/ollama/blob/HEAD/LICENSE
Copyright: Copyright (c) Ollama
ShortDescription: Get up and running with large language models locally.
Tags:
- ai
- chatbot
- deepseek
- large-language-model
- llama
- llm
- mistral
- qwen
ReleaseNotes: |-
  New models
  - DeepSeek-OCR: DeepSeek-OCR uses optical 2D mapping to compress long contexts, achieving high OCR precision with reduced vision tokens and demonstrating practical value in document processing.
  - Cogito-V2.1: instruction tuned generative models, currently the best open-weight LLM by a US company
  DeepSeek-OCR
  DeepSeek-OCR is now available on Ollama. Example inputs:
  ollama run deepseek-ocr "/path/to/image\n<|grounding|>Given the layout of the image."
  ollama run deepseek-ocr "/path/to/image\nFree OCR."
  ollama run deepseek-ocr "/path/to/image\nParse the figure."
  ollama run deepseek-ocr "/path/to/image\nExtract the text in the image."
  ollama run deepseek-ocr "/path/to/image\n<|grounding|>Convert the document to markdown."
  New bench tool
  Ollama's GitHub repo now includes a bench tool that can be used to test model performance. For the time being this is a separate tool that can be built in the Ollama GitHub repository:
  First, install Go. Then from the root of the Ollama repository run:
  go run ./cmd/bench.go -model gpt-oss:20b
  For more information see the tool's documentation
  What's Changed
  - DeepSeek-OCR is now supported
  - DeepSeek-V3.1 architecture is now supported in Ollama's engine
  - Fixed performance issues that arose in Ollama 0.12.11 on CUDA
  - Fixed issue where Linux install packages were missing required Vulkan libraries
  - Improved CPU and memory detection while in containers/cgroups
  - Improved VRAM information detection for AMD GPUs
  - Improved KV cache performance to no longer require defragmentation
  New Contributors
  - @lnicola made their first contribution in https://github.com/ollama/ollama/pull/13096
  - @vignesh1507 made their first contribution in https://github.com/ollama/ollama/pull/13078
  - @pierwill made their first contribution in https://github.com/ollama/ollama/pull/12995
  - @jjuliano made their first contribution in https://github.com/ollama/ollama/pull/11877
  - @omahs made their first contribution in https://github.com/ollama/ollama/pull/10683
  - @SiLeader made their first contribution in https://github.com/ollama/ollama/pull/10292
  - @ssam18 made their first contribution in https://github.com/ollama/ollama/pull/13124
  - @seolyam made their first contribution in https://github.com/ollama/ollama/pull/13116
  Full Changelog: https://github.com/ollama/ollama/compare/v0.12.11...v0.13.0-rc0
ReleaseNotesUrl: https://github.com/ollama/ollama/releases/tag/v0.13.0
Documentations:
- DocumentLabel: Documentation
  DocumentUrl: https://docs.ollama.com/
ManifestType: defaultLocale
ManifestVersion: 1.10.0

# Created with komac v2.12.1
# yaml-language-server: $schema=https://aka.ms/winget-manifest.defaultLocale.1.10.0.schema.json

PackageIdentifier: Ollama.Ollama
PackageVersion: 0.11.7
PackageLocale: en-US
Publisher: Ollama
PublisherUrl: https://ollama.com/
PublisherSupportUrl: https://github.com/ollama/ollama/issues
PackageName: Ollama
PackageUrl: https://ollama.com/
License: MIT
LicenseUrl: https://github.com/ollama/ollama/blob/HEAD/LICENSE
Copyright: Copyright (c) Ollama
ShortDescription: Get up and running with large language models locally.
Tags:
- ai
- chatbot
- deepseek
- large-language-model
- llama
- llm
- mistral
- qwen
ReleaseNotes: |-
  DeepSeek-V3.1
  DeepSeek-V3.1 is now available to run via Ollama.
  This model supports hybrid thinking, meaning thinking can be enabled or disabled by setting think in Ollama's API:
  curl http://localhost:11434/api/chat -d '{
    "model": "deepseek-v3.1",
    "messages": [
      {
        "role": "user",
        "content": "why is the sky blue?"
      }
    ],
    "think": true
  }'
  In Ollama's CLI, thinking can be enabled or disabled by running the /set think or /set nothink commands.
  Turbo (in preview)
  DeepSeek-V3.1 has over 671B parameters, and so a large amount of VRAM is required to run it. Ollama's Turbo mode (in preview) provides access to powerful hardware in the cloud you can use to run the model.
  Turbo via Ollama's app
  Screenshot 2025-08-25 at 1 23 37 PM
  1. Download Ollama for macOS or Windows
  2. Select deepseek-v3.1:671b from the model selector
  3. Enable Turbo
  Turbo via Ollama's CLI and libraries
  1. Create an account on ollama.com/signup
  2. Follow the docs for Ollama's CLI to upload authenticate your Ollama installation
  3. Run the following:
  OLLAMA_HOST=ollama.com ollama run deepseek-v3.1
  For instructions on using Turbo with Ollama's Python and JavaScript library, see the docs
  What's Changed
  - Fixed issue where multiple models would not be loaded on CPU-only systems
  - Ollama will now work with models who skip outputting the initial<think> tag (e.g. DeepSeek-V3.1)
  - Fixed issue where text would be emitted when there is no opening <think> tag from a model
  - Fixed issue where tool calls containing { or } would not be parsed correctly
  New Contributors
  - @zoupingshi made their first contribution in #12028
  Full Changelog: v0.11.6...v0.11.7
ReleaseNotesUrl: https://github.com/ollama/ollama/releases/tag/v0.11.7
Documentations:
- DocumentLabel: Documentation
  DocumentUrl: https://github.com/ollama/ollama/tree/main/docs
ManifestType: defaultLocale
ManifestVersion: 1.10.0

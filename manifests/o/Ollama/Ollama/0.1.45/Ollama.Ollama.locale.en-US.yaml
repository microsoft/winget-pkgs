# Created with YamlCreate.ps1 v2.4.1 Dumplings Mod $debug=QUSU.CRLF.7-4-2.Win32NT
# yaml-language-server: $schema=https://aka.ms/winget-manifest.defaultLocale.1.6.0.schema.json

PackageIdentifier: Ollama.Ollama
PackageVersion: 0.1.45
PackageLocale: en-US
Publisher: Ollama
PublisherUrl: https://ollama.com/
PublisherSupportUrl: https://github.com/ollama/ollama/issues
# PrivacyUrl:
# Author:
PackageName: Ollama
PackageUrl: https://ollama.com/
License: MIT
LicenseUrl: https://github.com/ollama/ollama/blob/main/LICENSE
Copyright: Copyright (c) Ollama
# CopyrightUrl:
ShortDescription: Get up and running with large language models locally.
# Description:
# Moniker:
Tags:
- llama
- llama2
- llm
- llms
- mistral
ReleaseNotes: |-
  New models
  - DeepSeek-Coder-V2: A 16B & 236B open-source Mixture-of-Experts code language model that achieves performance comparable to GPT4-Turbo in code-specific tasks.
  ollama show
  ollama show will now show model details such as context length, parameters, embedding size, license and more:
  % ollama show llama3
    Model
    	arch            	llama
    	parameters      	8.0B
    	quantization    	Q4_0
    	context length  	8192
    	embedding length	4096

    Parameters
    	num_keep	24
    	stop    	"<|start_header_id|>"
    	stop    	"<|end_header_id|>"
    	stop    	"<|eot_id|>"

    License
    	META LLAMA 3 COMMUNITY LICENSE AGREEMENT
    	Meta Llama 3 Version Release Date: April 18, 2024
  What's Changed
  - ollama show <model> will now show model information such as context window size
  - Model loading on Windows with CUDA GPUs is now faster
  - Setting seed in the /v1/chat/completions OpenAI compatibility endpoint no longer changes temperature
  - Enhanced GPU discovery and multi-gpu support with concurrency
  - The Linux install script will now skip searching for network devices
  - Introduced a workaround for AMD Vega RX 56 SDMA support on Linux
  - Fix memory prediction for deepseek-v2 and deepseek-coder-v2 models
  - api/show endpoint returns extensive model metadata
  - GPU configuration variables are now reported in ollama serve
  - Update Linux ROCm to v6.1.1
  New Contributors
  - @jayson-cloude made their first contribution in https://github.com/ollama/ollama/pull/4972
  Full Changelog: https://github.com/ollama/ollama/compare/v0.1.44...v0.1.45
ReleaseNotesUrl: https://github.com/ollama/ollama/releases/tag/v0.1.45
# PurchaseUrl:
# InstallationNotes:
Documentations:
- DocumentLabel: Documentation
  DocumentUrl: https://github.com/ollama/ollama/tree/main/docs
ManifestType: defaultLocale
ManifestVersion: 1.6.0

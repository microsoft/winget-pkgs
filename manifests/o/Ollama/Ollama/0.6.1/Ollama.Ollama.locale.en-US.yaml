# Created with YamlCreate.ps1 Dumplings Mod
# yaml-language-server: $schema=https://aka.ms/winget-manifest.defaultLocale.1.9.0.schema.json

PackageIdentifier: Ollama.Ollama
PackageVersion: 0.6.1
PackageLocale: en-US
Publisher: Ollama
PublisherUrl: https://ollama.com/
PublisherSupportUrl: https://github.com/ollama/ollama/issues
PackageName: Ollama
PackageUrl: https://ollama.com/
License: MIT
LicenseUrl: https://github.com/ollama/ollama/blob/main/LICENSE
Copyright: Copyright (c) Ollama
ShortDescription: Get up and running with large language models locally.
Tags:
- ai
- chatbot
- deepseek
- large-language-model
- llama
- llm
- mistral
- qwen
ReleaseNotes: |-
  What's Changed
  - Fixed issues where gemma3 would crash with "out of memory (OOM) errors" by improving memory estimation
  - Loading an invalid model with /load will no longer exit ollama run
  - Added Ctrl+P and Ctrl+N hotkeys for ollama run
  - New ollama show -v or ollama show --verbose that will print additional model data
  - Improved sampling parameters such as temperature and top_k to behave similar to other implementations
  - New --verbose flag for ollama show that prints additional model data
  New Contributors
  - @Shane-XB-Qian made their first contribution in https://github.com/ollama/ollama/pull/9136
  - @13rac1 made their first contribution in https://github.com/ollama/ollama/pull/9643
  Full Changelog: https://github.com/ollama/ollama/compare/v0.6.0...v0.6.1
ReleaseNotesUrl: https://github.com/ollama/ollama/releases/tag/v0.6.1
Documentations:
- DocumentLabel: Documentation
  DocumentUrl: https://github.com/ollama/ollama/tree/main/docs
ManifestType: defaultLocale
ManifestVersion: 1.9.0

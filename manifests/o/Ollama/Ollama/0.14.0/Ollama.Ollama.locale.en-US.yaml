# Created with YamlCreate.ps1 Dumplings Mod
# yaml-language-server: $schema=https://aka.ms/winget-manifest.defaultLocale.1.12.0.schema.json

PackageIdentifier: Ollama.Ollama
PackageVersion: 0.14.0
PackageLocale: en-US
Publisher: Ollama
PublisherUrl: https://ollama.com/
PublisherSupportUrl: https://github.com/ollama/ollama/issues
PackageName: Ollama
PackageUrl: https://ollama.com/
License: MIT
LicenseUrl: https://github.com/ollama/ollama/blob/HEAD/LICENSE
Copyright: Copyright (c) Ollama
ShortDescription: Get up and running with large language models locally.
Tags:
- ai
- chatbot
- deepseek
- large-language-model
- llama
- llm
- mistral
- qwen
ReleaseNotes: |-
  What's Changed
  - ollama run --experimental CLI will now open a new Ollama CLI that includes an agent loop and the bash tool
  - Anthropic API compatibility: support for the /v1/messages API
  - A new REQUIRES command for the Modelfile allows declaring which version of Ollama is required for the model
  - For older models, Ollama will avoid an integer underflow on low VRAM systems during memory estimation
  - More accurate VRAM measurements for AMD iGPUs
  - Ollama's app will now highlight swift soure code
  - An error will now return when embeddings return NaN or -Inf
  - Ollama's Linux install bundles files now use zst compression
  - New experimental support for image generation models, powered by MLX
  New Contributors
  - @Vallabh-1504 made their first contribution in https://github.com/ollama/ollama/pull/13550
  - @majiayu000 made their first contribution in https://github.com/ollama/ollama/pull/13596
  - @harrykiselev made their first contribution in https://github.com/ollama/ollama/pull/13615
  Full Changelog: https://github.com/ollama/ollama/compare/v0.13.5...v0.14.0-rc2
ReleaseNotesUrl: https://github.com/ollama/ollama/releases/tag/v0.14.0
Documentations:
- DocumentLabel: Documentation
  DocumentUrl: https://docs.ollama.com/
ManifestType: defaultLocale
ManifestVersion: 1.12.0

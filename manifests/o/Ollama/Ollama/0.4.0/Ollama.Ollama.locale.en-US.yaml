# Created with YamlCreate.ps1 v2.4.1 Dumplings Mod $debug=QUSU.CRLF.7-4-5.Win32NT
# yaml-language-server: $schema=https://aka.ms/winget-manifest.defaultLocale.1.6.0.schema.json

PackageIdentifier: Ollama.Ollama
PackageVersion: 0.4.0
PackageLocale: en-US
Publisher: Ollama
PublisherUrl: https://ollama.com/
PublisherSupportUrl: https://github.com/ollama/ollama/issues
# PrivacyUrl:
# Author:
PackageName: Ollama
PackageUrl: https://ollama.com/
License: MIT
LicenseUrl: https://github.com/ollama/ollama/blob/main/LICENSE
Copyright: Copyright (c) Ollama
# CopyrightUrl:
ShortDescription: Get up and running with large language models locally.
# Description:
# Moniker:
Tags:
- llama
- llama2
- llm
- llms
- mistral
ReleaseNotes: |-
  Llama 3.2 Vision
  Ollama 0.4 adds support for Llama 3.2 Vision. After upgrading or downloading Ollama, run:
  ollama run llama3.2-vision
  For the larger, 90B version of the model, run:
  ollama run llama3.2-vision:90b
  What's changed
  - Support for Llama 3.2 Vision (i.e. Mllama) architecture
  - Improved performance on new generation NVIDIA graphics cards (e.g. RTX 40 series)
  - Sending follow on requests to vision models will now be much faster
  - Fixed issues where stop sequences would not be detected correctly
  - Ollama can now import models from Safetensors without a Modelfile when running ollama create my-model
  - Fixed issue where redirecting output to a file on Windows would cause invalid characters to be written
  - Fixed issue where invalid model data would cause Ollama to error
  Full Changelog: https://github.com/ollama/ollama/compare/v0.3.14...v0.4.0
ReleaseNotesUrl: https://github.com/ollama/ollama/releases/tag/v0.4.0
# PurchaseUrl:
# InstallationNotes:
Documentations:
- DocumentLabel: Documentation
  DocumentUrl: https://github.com/ollama/ollama/tree/main/docs
ManifestType: defaultLocale
ManifestVersion: 1.6.0

# Created with YamlCreate.ps1 Dumplings Mod
# yaml-language-server: $schema=https://aka.ms/winget-manifest.defaultLocale.1.10.0.schema.json

PackageIdentifier: Ollama.Ollama
PackageVersion: 0.12.8
PackageLocale: en-US
Publisher: Ollama
PublisherUrl: https://ollama.com/
PublisherSupportUrl: https://github.com/ollama/ollama/issues
PackageName: Ollama
PackageUrl: https://ollama.com/
License: MIT
LicenseUrl: https://github.com/ollama/ollama/blob/HEAD/LICENSE
Copyright: Copyright (c) Ollama
ShortDescription: Get up and running with large language models locally.
Tags:
- ai
- chatbot
- deepseek
- large-language-model
- llama
- llm
- mistral
- qwen
ReleaseNotes: |-
  What's Changed
  - qwen3-vl performance improvements, including flash attention support by default
  - qwen3-vl will now output less leading whitespace in the response when thinking
  - Fixed issue where deepseek-v3.1 thinking could not be disabled in Ollama's new app
  - Fixed issue where qwen3-vl would fail to interpret images with transparent backgrounds
  - Ollama will now stop running a model before removing it via ollama rm
  - Fixed issue where prompt processing would be slower on Ollama's engine
  - Ignore unsupported iGPUs when doing device discovery on Windows
  New Contributors
  - @athshh made their first contribution in https://github.com/ollama/ollama/pull/12822
  Full Changelog: https://github.com/ollama/ollama/compare/v0.12.7...v0.12.8
ReleaseNotesUrl: https://github.com/ollama/ollama/releases/tag/v0.12.8
Documentations:
- DocumentLabel: Documentation
  DocumentUrl: https://docs.ollama.com/
ManifestType: defaultLocale
ManifestVersion: 1.10.0

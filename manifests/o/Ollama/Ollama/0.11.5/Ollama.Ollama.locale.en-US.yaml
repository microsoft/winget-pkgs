# Created with YamlCreate.ps1 Dumplings Mod
# yaml-language-server: $schema=https://aka.ms/winget-manifest.defaultLocale.1.10.0.schema.json

PackageIdentifier: Ollama.Ollama
PackageVersion: 0.11.5
PackageLocale: en-US
Publisher: Ollama
PublisherUrl: https://ollama.com/
PublisherSupportUrl: https://github.com/ollama/ollama/issues
PackageName: Ollama
PackageUrl: https://ollama.com/
License: MIT
LicenseUrl: https://github.com/ollama/ollama/blob/HEAD/LICENSE
Copyright: Copyright (c) Ollama
ShortDescription: Get up and running with large language models locally.
Tags:
- ai
- chatbot
- deepseek
- large-language-model
- llama
- llm
- mistral
- qwen
ReleaseNotes: |-
  What's Changed
  - Performance improvements for the gpt-oss models
  - Improved multi-GPU scheduling and reduced VRAM allocation when using more than 2 GPUs
  - Ollama's new app will now remember default selections for default model, Turbo and Web Search between restarts
  - Fix error when parsing bad harmony tool calls
  - OLLAMA_FLASH_ATTENTION=1 will also enable flash attention for pure-CPU models
  - Fixed OpenAI-compatible API not supporting reasoning_effort
  - Reduced size of installation on Windows and Linux
  - New Memory Management by @jessegross in https://github.com/ollama/ollama/pull/11090
  New Contributors
  - @vorburger made their first contribution in https://github.com/ollama/ollama/pull/11755
  - @dan-and made their first contribution in https://github.com/ollama/ollama/pull/10678
  - @youzichuan made their first contribution in https://github.com/ollama/ollama/pull/11880
  Full Changelog: https://github.com/ollama/ollama/compare/v0.11.4...v0.11.5-rc1
ReleaseNotesUrl: https://github.com/ollama/ollama/releases/tag/v0.11.5
Documentations:
- DocumentLabel: Documentation
  DocumentUrl: https://github.com/ollama/ollama/tree/main/docs
ManifestType: defaultLocale
ManifestVersion: 1.10.0

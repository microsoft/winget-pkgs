# Created with YamlCreate.ps1 Dumplings Mod
# yaml-language-server: $schema=https://aka.ms/winget-manifest.defaultLocale.1.6.0.schema.json

PackageIdentifier: Ollama.Ollama
PackageVersion: 0.4.5
PackageLocale: en-US
Publisher: Ollama
PublisherUrl: https://ollama.com/
PublisherSupportUrl: https://github.com/ollama/ollama/issues
PackageName: Ollama
PackageUrl: https://ollama.com/
License: MIT
LicenseUrl: https://github.com/ollama/ollama/blob/main/LICENSE
Copyright: Copyright (c) Ollama
ShortDescription: Get up and running with large language models locally.
Tags:
- llama
- llama2
- llm
- llms
- mistral
ReleaseNotes: |-
  What's Changed
  - Fixed issue where HTTPS_PROXY and HTTP_PROXY environment variables would have no effect
  - Ollama will now accept X-Stainless-Retry-Count used by many OpenAI API clients
  - Fix issue where importing certain GGUF files would result in the incorrect quantization level
  - ollama push will now print the uploaded model URL on ollama.com
  New Contributors
  - @rrg92 made their first contribution in https://github.com/ollama/ollama/pull/7438
  - @oza6ut0ne made their first contribution in https://github.com/ollama/ollama/pull/6910
  - @josStorer made their first contribution in https://github.com/ollama/ollama/pull/4118
  - @patcher9 made their first contribution in https://github.com/ollama/ollama/pull/7811
  - @erikos made their first contribution in https://github.com/ollama/ollama/pull/3591
  - @sunnybak made their first contribution in https://github.com/ollama/ollama/pull/7831
  Full Changelog: https://github.com/ollama/ollama/compare/v0.4.4...v0.4.5
ReleaseNotesUrl: https://github.com/ollama/ollama/releases/tag/v0.4.5
Documentations:
- DocumentLabel: Documentation
  DocumentUrl: https://github.com/ollama/ollama/tree/main/docs
ManifestType: defaultLocale
ManifestVersion: 1.6.0

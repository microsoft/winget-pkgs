# Created with YamlCreate.ps1 v2.4.1 $debug=NVS1.CRLF.7-4-5.Win32NT
# yaml-language-server: $schema=https://aka.ms/winget-manifest.defaultLocale.1.6.0.schema.json

PackageIdentifier: Mozilla.llamafile
PackageVersion: 0.8.14
PackageLocale: en-US
Publisher: Mozilla Ocho
PublisherUrl: https://github.com/Mozilla-Ocho
PublisherSupportUrl: https://github.com/Mozilla-Ocho/llamafile/issues
# PrivacyUrl:
# Author:
PackageName: llamafile
PackageUrl: https://github.com/Mozilla-Ocho/llamafile
License: Apache-2.0
LicenseUrl: https://github.com/Mozilla-Ocho/llamafile/blob/HEAD/LICENSE
Copyright: Copyright 2024 Mozilla Foundation
# CopyrightUrl:
ShortDescription: Distribute and run LLMs with a single file.
Description: |-
  llamafile lets you distribute and run LLMs with a single file.
  Our goal is to make open LLMs much more accessible to both developers and end users. We're doing that by combining llama.cpp with Cosmopolitan Libc into one framework that collapses all the complexity of LLMs down to a single-file executable (called a "llamafile") that runs locally on most computers, with no installation.
# Moniker:
Tags:
- ai
- large-language-model
- llama
- llm
ReleaseNotes: |-
  This release introduces our new CLI chatbot interface. It supports multi-line input using triple quotes. It will syntax highlight Python, C, C++, Java, and JavaScript code.
  This chatbot is now the default mode of operation. When you launch llamafile without any special arguments, the chatbot will be launched in the foreground, and the server will be launched in the background. You can use the --chat and --server flags to disambiguate this behavior if you only want one of them.
  - a384fd7 Create ollama inspired cli chatbot
  - 63205ee Add syntax highlighting to chatbot
  - 7b395be Introduce new --chat flag for chatbot
  - 28e98b6 Show prompt loading progress in chatbot
  - 4199dae Make chat+server hybrid the new default mode
  The whisperfile server now lets you upload mp3/ogg/flac.
  - 74dfd21 Rewrite audio file loader code
  - 7517a5f whisperfile server: convert files without ffmpeg (#568)
  Other improvements have been made.
  - d617c0b Added vision support to api_like_OAI (#524)
  - 726f6e8 Enable gpu support in llamafile-bench (#581)
  - c7c4d65 Speed up KV in llamafile-bench
  - 2c940da Make replace_all() have linear complexity
  - fa4c4e7 Use bf16 kv cache when it's faster
  - 20fe696 Upgrade to Cosmopolitan 3.9.4
  - c44664b Always favor fp16 arithmetic in tinyBLAS
  - 98eff09 Quantize TriLM models using Q2_K_S (#552)
ReleaseNotesUrl: https://github.com/Mozilla-Ocho/llamafile/releases/tag/0.8.14
# PurchaseUrl:
# InstallationNotes:
# Documentations:
ManifestType: defaultLocale
ManifestVersion: 1.6.0

# Created with komac v2.13.0
# yaml-language-server: $schema=https://aka.ms/winget-manifest.defaultLocale.1.10.0.schema.json

PackageIdentifier: mostlygeek.llama-swap
PackageVersion: '167'
PackageLocale: en-US
Publisher: mostlygeek
PublisherUrl: https://mostlygeek.com/
PublisherSupportUrl: https://github.com/mostlygeek/llama-swap/issues
PackageName: llama-swap
PackageUrl: https://github.com/mostlygeek/llama-swap
License: MIT
LicenseUrl: https://github.com/mostlygeek/llama-swap/blob/HEAD/LICENSE.md
Copyright: Copyright (c) 2024 Benson Wong
ShortDescription: Model swapping for llama.cpp
Description: llama-swap is a light weight, transparent proxy server that provides automatic model swapping to llama.cpp's server.
Moniker: llama-swap
Tags:
- golang
- llama
- llamacpp
- localllama
- localllm
- openai
- openai-api
- vllm
ReleaseNotes: |-
  This release adds cmd/wol-proxy, a Wake-on-LAN proxy for llama-swap. If llama-swap lives on a high idle wattage server that suspends after an idle period, wol-proxy will automatically wake that server up and then reverse proxy the requests.
  A niche use case but hopefully it will save a lot of wasted energy from idle GPUs.
  Changelog
  - c07179d cmd/wol-proxy: add wol-proxy (#352)
  - 7ff5063 Update README for setup instructions clarity [skip ci]
  - 9fc0431 Clean up and Documentation (#347) [skip ci]
ReleaseNotesUrl: https://github.com/mostlygeek/llama-swap/releases/tag/v167
ManifestType: defaultLocale
ManifestVersion: 1.10.0

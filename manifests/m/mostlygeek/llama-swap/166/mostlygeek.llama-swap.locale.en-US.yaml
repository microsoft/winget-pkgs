# Created with komac v2.13.0
# yaml-language-server: $schema=https://aka.ms/winget-manifest.defaultLocale.1.10.0.schema.json

PackageIdentifier: mostlygeek.llama-swap
PackageVersion: '166'
PackageLocale: en-US
Publisher: mostlygeek
PublisherUrl: https://mostlygeek.com/
PublisherSupportUrl: https://github.com/mostlygeek/llama-swap/issues
PackageName: llama-swap
PackageUrl: https://github.com/mostlygeek/llama-swap
License: MIT
LicenseUrl: https://github.com/mostlygeek/llama-swap/blob/HEAD/LICENSE.md
Copyright: Copyright (c) 2024 Benson Wong
ShortDescription: Model swapping for llama.cpp
Description: llama-swap is a light weight, transparent proxy server that provides automatic model swapping to llama.cpp's server.
Moniker: llama-swap
Tags:
- golang
- llama
- llamacpp
- localllama
- localllm
- openai
- openai-api
- vllm
ReleaseNotes: |-
  This release includes support for TLS certificates from contributor @dwrz!
  To use it:
  ./llama-swap --tls-cert-file /path/to/cert.pem --tls-key-file /path/to/key.pem ...
  Generating a self-signed certificate:
  openssl req -x509 -newkey rsa:4096 -keyout key.pem -out cert.pem -days 365 -nodes
  Changelog
  - 6516532 Add optional TLS support (#340)
  - d58a8b8 Refactor to use httputil.ReverseProxy (#342)
  - caf9e98 Fix race conditions in proxy.Process (#349)
ReleaseNotesUrl: https://github.com/mostlygeek/llama-swap/releases/tag/v166
ManifestType: defaultLocale
ManifestVersion: 1.10.0

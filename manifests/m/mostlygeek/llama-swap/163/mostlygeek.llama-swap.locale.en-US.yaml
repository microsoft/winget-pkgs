# Created with komac v2.13.0
# yaml-language-server: $schema=https://aka.ms/winget-manifest.defaultLocale.1.10.0.schema.json

PackageIdentifier: mostlygeek.llama-swap
PackageVersion: '163'
PackageLocale: en-US
Publisher: mostlygeek
PublisherUrl: https://mostlygeek.com/
PublisherSupportUrl: https://github.com/mostlygeek/llama-swap/issues
PackageName: llama-swap
PackageUrl: https://github.com/mostlygeek/llama-swap
License: MIT
LicenseUrl: https://github.com/mostlygeek/llama-swap/blob/HEAD/LICENSE.md
Copyright: Copyright (c) 2024 Benson Wong
ShortDescription: Model swapping for llama.cpp
Description: llama-swap is a light weight, transparent proxy server that provides automatic model swapping to llama.cpp's server.
Moniker: llama-swap
Tags:
- golang
- llama
- llamacpp
- localllama
- localllm
- openai
- openai-api
- vllm
ReleaseNotes: |-
  This release includes two new features:
  - model macros (#330): macros can now be defined as part of a model's configuration. These take precedence over macros defined at the global level.
  - model metadata (#333): metadata can now be defined in a model's configuration. This is a schema-less object that supports integers, floats, bools, strings, arrays and child objects. metadata fields also support macro substitution. Metadata is only used in the v1/models endpoint under a new JSON key: meta.llamaswap.
  Other smaller changes:
  - macro values can be any integer, string, bools, or float types. This enhancement makes JSON encoding of metadata with macros behave as expected. Previously macro values could only be strings.
  Changelog
  - 70930e4 proxy: add support for user defined metadata in model configs (#333)
  - 1f61791 proxy/config: add model level macros (#330)
  - 216c40b proxy/config: create config package and migrate configuration (#329)
ReleaseNotesUrl: https://github.com/mostlygeek/llama-swap/releases/tag/v163
ManifestType: defaultLocale
ManifestVersion: 1.10.0
